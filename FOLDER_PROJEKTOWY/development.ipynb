{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJEKT\n",
    "\n",
    "## Co nas interesuje?\n",
    "\n",
    "Głównie notatniki examine_model i augmentation_samples.\n",
    "Zawartość pierwszego z nich jest poniżej w zmodyfikowanej formie i daje czytelny schemat działania całego programu.\n",
    "\n",
    "Jako \"model\" autorzy rozumieją nakładkę klasyfikującą feature'y wychodzące z resneta/alexneta etc. Są nimi zapisane w models.py pipeliny klasyfikujące np. FV-SVC.\n",
    "Model klasyfikujący jest optymalizowany przez gridsearch w hyperparameters.py na danych z feature_matrix.npy który wychodzi z extract_features.py.\n",
    "\n",
    "Samo extract_features.py używa compute_feature_matrix() pochodzącej z features.py. Wewnątrz wspomnianej funkcji kluczowa jest  extract_features(), która dla danego \"extractora\" (np. alexnet) i obrazka/batcha N obrazków faktycznie zwraca N wektorów w latencie.\n",
    "\n",
    "\n",
    "## Czego potrzeba na dniach 8-13.05\n",
    "\n",
    "Raczej nie ma co liczyć, że będziemy mieli nasze rozwiązania wimplementowane w strukturę całego projektu, więc ograniczymy się do pusuzczenia działającego runa (parę zrobionych w augmentation_samples patchy -> VIT -> DS-MIL / FV-RF -> wynik) w tym notatniku.\n",
    "Piętro niżej jest już zaimportowany vit_l_32. Natywne funkcje projektu nie współpracują z importowanym vitem. \n",
    "\n",
    "Potrzeba nam: \n",
    "- Kilka patchy zrobionych w augmentation_samples downscalowanych do 224x224 (vim przyjmuje (N, 3, 224, 224))\n",
    "- Gotowych do klasyfikacji DS-MIL i FV-RF jako .pkl \n",
    "(aktualne modele nauczone są na innych rozmiarach wejść \"3D tensor (N, W * H, C)\", podczas gdy vim_l_32 wypuszcza (N, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/szymon/Dokumenty/Studia/Sezon 7/rozpoznawanie obrazów/fungus/FOLDER_PROJEKTOWY/development.ipynb Komórka 1\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/szymon/Dokumenty/Studia/Sezon%207/rozpoznawanie%20obraz%C3%B3w/fungus/FOLDER_PROJEKTOWY/development.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/szymon/Dokumenty/Studia/Sezon%207/rozpoznawanie%20obraz%C3%B3w/fungus/FOLDER_PROJEKTOWY/development.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vit \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mvit_b_16(pretrained\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "vit = models.vit_b_16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: \\ \n",
      "Found conflicts! Looking for incompatible packages.\n",
      "This can take several minutes.  Press CTRL-C to abort.\n",
      "Examining conflict for mkl-service bkcharts dask numba patsy cfitsio imagecode/ - \\ "
     ]
    }
   ],
   "source": [
    "conda install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from skimage.io import imread\n",
    "from matplotlib import pyplot as plt\n",
    "from pipeline import features\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Compose\n",
    "from dataset.normalization import get_normalization_transform\n",
    "from util.augmentation import NumpyToTensor\n",
    "from pathlib import Path\n",
    "from dataset import FungusDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to adjust this accroding to your setup\n",
    "model_path = './best_model.pkl' #FV-SVC, potrzeba nam jednego z DS-MIL i jednego FV-RF\n",
    "patch_path = './patch.png'      #Potrzebujemy jednego patcha z augmentation_samples.ipynb\n",
    "normalization_params_path = Path('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and inspect model and image patch\n",
    "model = joblib.load(model_path)\n",
    "print(model)\n",
    "patch = imread(patch_path)\n",
    "plt.imshow(patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize image\n",
    "transform = [\n",
    "    NumpyToTensor(),\n",
    "    get_normalization_transform(normalization_params_path),\n",
    "]\n",
    "transform = Compose(transform)\n",
    "patch = transform(patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from patch\n",
    "device = features.get_cuda()\n",
    "patch = patch.unsqueeze(0)\n",
    "print(patch.shape)\n",
    "# best model uses alexnet, so let's load it\n",
    "# extractor = models.alexnet(pretrained=True).features.eval().to(device)\n",
    "# features_matrix = features.extract_features(patch, device, extractor)\n",
    "# features_matrix = features_matrix.detach().numpy()\n",
    "\n",
    "# Używają alexneta. Implementuję w jego miejsce VIT'a vit_l_32 https://pytorch.org/vision/main/models/vision_transformer.html\n",
    "vit = models.vit_l_32(pretrained=True)\n",
    "vit.heads = nn.Identity() # Usuwanie klasyfikacji końcowej\n",
    "features_matrix = vit(patch) \n",
    "features_matrix = features_matrix.detach().numpy()\n",
    "print(features_matrix.shape)\n",
    "# Wynik rozmiaru Nx1024 gdzie N to ilość patchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict classification\n",
    "result = model.predict(features_matrix)\n",
    "FungusDataset.NUMBER_TO_FUNGUS[result[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fungus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
